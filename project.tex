\documentclass[12pt]{article}

\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}

\usepackage[L7x,T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[lithuanian,english]{babel}

\graphicspath{ {./figures/} }

\setstretch{1.5}

\thispagestyle{empty}

\begin{document}
	\begin{center}

	    \vspace*{1cm}
	    \Large
	    Vilnius University

		Mathematics and Informatics Faculty

		Institute of Informatics 

		Bioinformatics study program
	    
        \vspace*{2cm}
        \Large
		\textbf{Protein thermostability prediction using 
		sequence representations from protein 
		language models}

	\end{center}

	\begin{flushright}

		\vspace*{2cm}
        \large
        Author: Ieva Pudžiuvelytė

        Supervisor: Kliment Olechnovič, PhD 
        
	\end{flushright}

	\begin{center}
		\vspace*{4cm}
        \large
        Course work project
        
        \vspace*{2cm}
        \large
        Vilnius, 2022
	\end{center}
	
	\newpage

	\tableofcontents

	\newpage
	
	\section{Introduction}

	This work is a prolongation of the previous work - the model 
	that performed binary classification into thermostability 
	classes. The model took ESM-1b protein embeddings as input 
	and provided prediction for each protein, how likely it 
	belongs to the thermostable class.
        
	\normalsize

	\newpage

	\section{Abstract in Lithuanian (Santrauka)}

	\begin{otherlanguage}{lithuanian}
		
	\end{otherlanguage}
	
	\newpage

	\section{Theory}

	\subsection{ESM-1b embeddings}

	\subsection{ProtTrans embeddings}

	\newpage

	\section{Methods}

	\subsection{Objective of this work}

	The main objective of this work is to analyse 
	which numerical representation of proteins is the most 
	suitable to use as input for the neural network 
	model to make binary protein classification into thermostability
	classes. Additionally, it was decided to try model 
	architectures with one or two hidden layers and evaluate 
	whether the different architecture improves the 
	performance.

	\subsection{Data set}

	The data set, which was used for the research of principal components' 
	usage as input this work, was 
	used the same as for the training, validation, and testing of 
	the single-layer perceptron (SLP) with mean ESM-1b 
	representations in the previous work. Although, for the 
	research of other representations, the data set was 
	filtered out of identical sequences to 
	get more accurate evaluations.

	\begin{table}[h!]
		\caption{Number of sequences with embeddings before and after 
		filtering the data set}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Subset & Original & After filtering \\
			\hline 
			Training & 284309 & 283360 \\
			Validation & 65156 & 63158 \\
			Testing & 73662 & 73308 \\
			\hline    
		\end{tabular}
		\label{table:numberEmbeddings}
	\end{table}

	\begin{table}[h!]
		\caption{Number of sequences with embeddings in each class 
		before and after filtering the data set}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Class & Original & After filtering \\
			\hline 
			0 & 216595 & 212129 \\
			1 & 212729 & 207697 \\
			\hline    
		\end{tabular}
		\label{table:numberEmbeddingsClasses}
	\end{table}

	\newpage
	
	\subsection{Correlation analysis of embeddings' components}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_correlation_high_corr.png}

		\caption{Plot of ESM-1b components that have got high correlation 
		coefficients with ProtTrans components}
		\label{figure:highCorrelationComponents}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_correlation_hist.png}

		\caption{Histogram of correlation coefficients between ESM-1b 
		and ProtTrans components}
		\label{figure:correlationComponentsHisto}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_correlation_max.png}

		\caption{Plot of ESM-1b components' maximum correlation coefficients with 
		ProtTrans components}
		\label{figure:correlationComponentsMax}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_correlation_mean.png}

		\caption{Plot of ESM-1b components' averaged  correlation coefficients 
		with ProtTrans components}
		\label{figure:correlationComponentsMean}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_PC_95_correlation_high_corr.png}

		\caption{Plot of ESM-1b principal components (explaining 
		95\% of data variation) that have got high correlation coefficients 
		with ProtTrans principal components (95\%)}
		\label{figure:highCorrelationComponentsPC95}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_PC_95_correlation_hist.png}

		\caption{Histogram of correlation coefficients between ESM-1b 
		and ProtTrans principal components (95\%)}
		\label{figure:correlationComponentsHistoPC95}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_PC_95_correlation_max.png}

		\caption{Plot of ESM-1b principal components' (95\%) maximum correlation 
		coefficients with ProtTrans principal components (95\%)}
		\label{figure:correlationComponentsMaxPC95}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_PC_95_correlation_mean.png}

		\caption{Plot of ESM-1b principal components' (95\%) averaged correlation 
		coefficients with ProtTrans principal components (95\%)}
		\label{figure:correlationComponentsMeanPC95}
	\end{figure}

	\newpage

	\subsection{Analysed representations}

	Firstly, principal components of ESM-1b and ProtTrans
	mean embeddings were retrieved and taken as input for the 
	model. In particular, principal components that explained 
	95 percent and 100 percent of the data variance were taken.
	The data set for these representations analysis was picked 
	the same as for the SLP analysis in the previous work.

	\begin{table}[h!]
		\caption{Sizes of the analysed principal components 
		vectors}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			 & ESM-1b & ProtTrans \\
			\hline 
			95\% & 540 & 453 \\
			100\% & 1280 & 1024 \\
			\hline    
		\end{tabular}
		\label{table:vectorsPCADimensions}
	\end{table}
	
	Furthermore, both protein language models - ESM-1b and 
	ProtTrans - provide 
	per token or per residue representations - each 
	amino acid of the protein gets 1280 or 1024-dimensional vector from
	ESM-1b or ProtTrans model respectively. Therefore, each protein is 
	originally represented by ${m \times n}$ matrix, 
	where ${m}$ is the number of dimensions of the chosen type of embedding
	and ${n}$ is the number of amino acids that compose the protein. 
	These representations were processed to get vectors of the same dimension 
	for each protein in the data set. The representations 
	that were included in the analysis:

	\begin{enumerate}
		\item Mean ESM-1b and ProtTrans 
		\item Joined mean ESM-1b and ProtTrans
		\item Normalised mean ESM-1b and ProtTrans
		\item Joined normalised mean ESM-1b and ProtTrans
		\item Median ESM-1b and ProtTrans
		\item Minimum, median, and maximum ESM-1b and ProtTrans
		\item Quantiles (including minimum and maximum) ESM-1b and ProtTrans
		\item Quantiles (including minimum and maximum) and mean ESM-1b and ProtTrans
		\item Octiles (including minimum and maximum) ESM-1b and ProtTrans
	\end{enumerate}

	\begin{table}[h!]
		\caption{Sizes of the analysed representations' vectors}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Representation & ESM-1b & ProtTrans \\
			\hline 
			Mean & 1280 & 1024 \\
			Joined mean & \multicolumn{2}{c|}{2304} \\
			Median & 1280 & 1024 \\
			Minimum, median, maximum & 3840 & 3072 \\
			Quantiles & 6400 & 5120 \\
			Quantiles and mean & 7680 & 6144 \\
			Octiles & 11520 & 9216 \\
			\hline    
		\end{tabular}
		\label{table:vectorsDimensions}
	\end{table}

	\newpage

	\subsection{Analysed architectures}

	\begin{table}[h!]
		\caption{Models that were tested with ESM-1b embeddings input}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Model & Number of hidden layers & Size of hidden layers \\
			\hline 
			C2H2\_h640-320 & 2 & 640, 320 \\
			C2H2\_h320-160 & 2 & 320, 160 \\
			C2H1\_h640 & 1 & 640 \\
			C2H1\_h320 & 1 & 320 \\
			C2H1\_h160 & 1 & 160 \\
			SLP\_ESM-1b & 0 & - \\
			\hline    
		\end{tabular}
		\label{table:modelArchitecturesESM}
	\end{table}

	\begin{table}[h!]
		\caption{Models that were tested with ProtTrans embeddings input}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Model & Number of hidden layers & Size of hidden layers \\
			\hline 
			C2H2\_h512-256 & 2 & 512, 256 \\
			C2H2\_h256-128 & 2 & 256, 128 \\
			C2H1\_h512 & 1 & 512 \\
			C2H1\_h256 & 1 & 256 \\
			C2H1\_h128 & 1 & 128 \\
			SLP\_ProtTrans & 0 & - \\
			\hline    
		\end{tabular}
		\label{table:modelArchitecturesPT}
	\end{table}

	\newpage

	\section{Results}

	\subsection{Representation analysis}

	\begin{table}[h!]
		\caption{The comparison of scores between models trained with ESM-1b
		and ProtTrans mean representations' principal components 
		that account for 95\% of the data set variance}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
						
			& ESM-1b & ProtTrans \\
			\hline 
			MCC & 0.699 & 0.767 \\
			Accuracy & 0.845 & 0.880 \\
			Loss & 0.383 & 0.442 \\
			Precision & 0.910 & 0.940 \\
			Recall & 0.768 & 0.813 \\
			ROC AUC & 0.901 & 0.945 \\
			\hline    
		\end{tabular}
		\label{table:comparisonESMandPTPC95}
	\end{table}

	\begin{table}[h!]
		\caption{The comparison of scores between models trained with ESM-1b
		and ProtTrans mean representations' principal components 
		that account for 100\% of the data set variance}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
						
			& ESM-1b & ProtTrans \\
			\hline 
			MCC & 0.698 & 0.766 \\
			Accuracy & 0.845 & 0.879 \\
			Loss & 0.382 & 0.443 \\
			Precision & 0.909 & 0.939 \\
			Recall & 0.767 & 0.812 \\
			ROC AUC & 0.901 & 0.943 \\
			\hline    
		\end{tabular}
		\label{table:comparisonESMandPTPC100}
	\end{table}

	By comparing SLP model's trained with ESM-1b embeddings results
	with results of SLP trained with ProtTrans, it was observed that 
	the latter model performs better (Table \ref{table:comparisonESMandPT}).

	\begin{table}[h!]
		\caption{Comparison of scores between models trained with ESM-1b
		and ProtTrans mean representations}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
						
			& ESM-1b & ProtTrans \\
			\hline 
			MCC & 0.843 & 0.901 \\
			Accuracy & 0.921 & 0.951 \\
			Loss & 0.208 & 0.128 \\
			Precision & 0.921 & 0.949 \\
			Recall & 0.917 & 0.949 \\
			ROC AUC & 0.979 & 0.990 \\
			\hline    
		\end{tabular}
		\label{table:comparisonESMandPT}
	\end{table}

	After joining ESM-1b and ProtTrans mean embeddings, an SLP was trained 
	using these joined representations. The results of this model were not 
	better than model's trained using only ProtTrans embeddings scores
	(Table \ref{table:comparisonESMandPT} and 
	\ref{table:comparisonJoinedAndJoinedNorm}). 

	\begin{table}[h!]
		\caption{The comparison of testing stage scores between 
		models trained with ESM-1b and ProtTrans mean 
		representations}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
						
			& Joined & Normalised joined \\
			\hline 
			MCC & 0.899 & 0.920 \\
			Accuracy & 0.949 & 0.960 \\
			Loss & 0.131 & 0.139 \\
			Precision & 0.945 & 0.954 \\
			Recall & 0.951 & 0.964 \\
			ROC AUC & 0.991 & 0.992 \\
			\hline    
		\end{tabular}
		\label{table:comparisonJoinedAndJoinedNorm}
	\end{table}

	\begin{table}[h!]
		\caption{The comparison of SLPs', which were 
		trained with normalised ESM-1b and ProtTrans mean 
		representations, testing stage scores}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 	
			& ESM-1b & ProtTrans \\
			\hline 
			MCC & 0.858 & 0.915 \\
			Accuracy & 0.929 & 0.957 \\
			Loss & 0.248 & 0.143 \\
			Precision & 0.923 & 0.951 \\
			Recall & 0.931 & 0.962 \\
			ROC AUC & 0.982 & 0.991 \\
			\hline    
		\end{tabular}
		\label{table:normalisedESMAndPTScores}
	\end{table}


	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.7]{SLP_ESM_003_diff_representations.png}

		\caption{Comparison of SLP models', which were trained with different
		ESM-1b representations, MCC and ROC AUC scores}
		\label{figure:scoresRepresentationsESM}
	\end{figure}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.7]{SLP_PT_003_diff_representations.png}

		\caption{Comparison of SLP models', which were trained with different
		ProtTrans representations, MCC and ROC AUC scores}
		\label{figure:scoresRepresentationsPT}
	\end{figure}

	\newpage

	\subsection{Architecture analysis}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.6]{MLP_ESM.png}

		\caption{Comparison of models', which were trained using 
		ESM-1b embeddings, MCC and ROC AUC scores}
		\label{figure:scoresMLP_ESM}
	\end{figure}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.6]{MLP_PT.png}

		\caption{Comparison of models', which were trained using 
		ProtTrans embeddings, MCC and ROC AUC scores}
		\label{figure:scoresMLP_PT}
	\end{figure}

	\newpage

	\section{Conclusions}

	\newpage

	\section{Availability}

	The code that was used to receive the results of this work can be found
	in the designated Github repository: 
	\href{https://github.com/ievapudz/Course_Work_Project}{https://github.com/ievapudz/Course\_Work\_Project}.

	\newpage
	
	\nocite{*}
	
	\normalsize

\bibliography{references} 
\bibliographystyle{ieeetr}

\end{document}
