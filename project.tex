\documentclass[12pt]{article}

\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}

\usepackage[L7x,T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[lithuanian,english]{babel}

\graphicspath{ {./figures/} }

\setstretch{1.5}

\thispagestyle{empty}

\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\begin{document}
	\begin{center}

	    \vspace*{1cm}
	    \Large
	    Vilnius University

		Mathematics and Informatics Faculty

		Institute of Informatics 

		Bioinformatics study program
	    
        \vspace*{2cm}
        \Large
		\textbf{Protein thermostability prediction using 
		sequence representations from protein 
		language models}

	\end{center}

	\begin{flushright}

		\vspace*{2cm}
        \large
        Author: Ieva Pudžiuvelytė

        Supervisor: Kliment Olechnovič, PhD 
        
	\end{flushright}

	\begin{center}
		\vspace*{4cm}
        \large
        Course work project
        
        \vspace*{2cm}
        \large
        Vilnius, 2023
	\end{center}
	
	\newpage

	\tableofcontents

	\newpage
	
	\section{Introduction}

	This work is a prolongation of the previous work - the model 
	that performed binary classification into thermostability 
	classes. The model, which was a single-layer perceptron (SLP), 
    took protein language model's ESM-1b \cite{rives2021biological} 
	protein embeddings as input 
	and provided prediction for each protein, how likely it 
	belongs to the thermostable class.

    The classes of thermostability were defined the following 
    way: proteins that were considered as stable in lower that 
    65 degrees of Celsius were labelled with '0' and the remaining
	proteins were labelled with '1'. 
    
    The classifier was trained on the data set 
	\cite{engqvist_martin_karl_magnus_2018_1175609} that contained 
	proteome identifiers, which were used 
    to collect proteins. The data set also contained information 
	about organism's growth temperature, therefore proteins that 
	belonged to a particular proteome were labelled accordingly. 

    Nevertheless the classifier showed promising results, yet an
    important downside of the developed method was emphasized -
    since ESM-1b embeddings generation was limited by the size 
    of the protein, the model could not provide predictions for 
    proteins that were longer than 1022 amino acids. For this 
    reason, it was decided to try ProtTrans 
	\cite{elnaggar2020prottrans} embeddings as an 
    input for the classification model.

    Furthermore, it was interesting to exploit not only mean 
    embeddings, but also to check, whether a different 
    connection of per residue embeddings would improve the 
    performance of the classification model.

    In addition to these fixed tasks, it was determined to 
    examine whether other variants of model architectures 
    would elaborate the classifier's results.

    The outcomes of this collection of tasks will 
    contribute in creation of the final definition of the method
    for binary protein classification into thermostability 
    classes.

	\normalsize

	\newpage

	\section{Abstract in Lithuanian (Santrauka)}

	\begin{otherlanguage}{lithuanian}
	
    Šis darbas yra ankstesnio darbo - binarinę baltymų klasifikaciją 
    pagal termostabilumą vykdančio modelio vystymo - tęsinys. Vystomas
    modelis buvo vieno sluoksnio perceptronas, kurio įvestis ESM-1b 
	\cite{rives2021biological}
    baltymų kalbos modelio generuojamos skaitinės reprezentacijos, o
    išvestis - prognozė kiekvienam baltymui, kaip tikėtina, kad jis 
    priklauso termostabilių baltymų klasei.
    
    Termostabilumo klasės buvo atskirtos 65 Celsijaus laipsnių riba:
    baltymai, kurie yra stabilūs žemesnėje nei nurodyta riba 
    temperatūroje, priskiriami klasei su žyme '0', o stabilūs 
    baltymai 65 laipsnių ir aukštesnėje temperatūroje yra 
    laikomi klasės '1' nariais.
    
    Minėtas klasifikatorius buvo apmokytas naudojant duomenų rinkinį, 
	kuriame
    saugomos organizmų augimo temperatūros 
	\cite{engqvist_martin_karl_magnus_2018_1175609}. Į duomenų rinkinį 
	taip pat
    buvo įtraukti organizmų proteomų identifikatoriai, kurie buvo
    panaudoti surinkti proteomui priklausančius baltymus ir juos 
    sužymėti pagal organizmo augimo temperatūrą.

    Nepaisant to, kad klasifikatorius pateikė neblogus rezultatus,
    testuojant metodą išryškėjo svarbus trūkumas. Kadangi ESM-1b
    skaitinių reprezentacijų kūrimas buvo apribotas baltymo ilgiu,
    nebuvo galima sugeneruoti reprezentacijų baltymams, kurie buvo 
    ilgesni nei 1022 aminorūgštys. Dėl šios priežasties buvo nutarta
    išmėginti ProtTrans \cite{elnaggar2020prottrans} skaitines 
	reprezentacijas kaip klasifikatoriaus
    įvestį, nes šis baltymų kalbos modelis buvo apmokytas be apribojimo 
    baltymo ilgiui. 

    Taip pat domino išmėginti ne tik suvidurkintas skaitines 
    reprezentacijas, bet ir išnaudoti galimybę išgauti kitaip
    apibendrintas kiekvienos aminorūgšties reprezentacijas bei 
    patikrinti, ar gauti kitokie įvesties vektoriai suteikia
    geresnius rezultatus. 
  
    Apart reprezentacijų analizės, taip pat buvo atlikti 
	eksperimentai
    patikrinimui, ar kitokios modelių architektūros turės
    ženklios įtakos modelio veikimui.
      
    Gautos analizių išvados suteiks naudingų įžvalgų galutinio
    metodo baltymų klasifikacijai pagal termostabilumą apibrėžimui.

	\end{otherlanguage}
	
	\newpage

	\section{Theory}

	\subsection{ProtTrans embeddings}

	ProtTrans \cite{elnaggar2020prottrans} is a collection 
	of protein language models 
	(LMs) that were trained to learn information about 
	proteins and encode it. ProtTrans embeddings are vector 
	representations taken from the last hidden state of 
	the protein LM. 

	In particular, ProtT5-XL model was used in this work and 
	exactly this model will be referred to by the name of 
	'ProtTrans'. Overall, this model has 24 layers. In particular, 
	the size of the hidden layer, from which the 
	embedding is taken, is 1024. This model was trained on 
	BFD-100 data set 
	and fine-tuned on UniRef50. Since ProtT5-XL model was 
	considered by the 
	authors as the best-performing model, it was chosen to be 
	applied in this work. Additionally, this model 
	does not have 
	positional encoding limit, which means that there is no 
	limitation for the protein's size to generate its embedding. 

	Furthermore, according to the article, in which ProtTrans
	project was published, ProtT5-XL models (trained on BFD-100 
	and UniRef50 data sets separately) 
	outperformed ESM-1b \cite{rives2021biological} model.

	\newpage

	\section{Methods}

	\subsection{Objective of this work}

	The main objective of this work is to analyse 
	which numerical representation of proteins is the most 
	suitable to use as input for the neural network 
	model that solves our thermostability prediction problem. 
	Additionally, it was decided to try model 
	architectures with one or two hidden layers and evaluate, 
	whether the different architecture improves the 
	performance.

	\subsection{Data set}

	In this work two types of data sets were used: for the analysis of 
	principal components' the data set was inherited from the previous 
	work, although the analysis of other representations was carried 
	out using the data set that was filtered from identically matching 
	sequences to get more accurate evaluations.

	\begin{table}[h!]
		\caption{Number of sequences with embeddings before and after 
		filtering the data set}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Subset & Original & After filtering \\
			\hline 
			Training & 284309 & 283360 \\
			Validation & 65156 & 63158 \\
			Testing & 73662 & 73308 \\
			\hline    
		\end{tabular}
		\label{table:numberEmbeddings}
	\end{table}

	\begin{table}[h!]
		\caption{Number of sequences with embeddings in each class 
		before and after filtering the data set}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Class & Original & After filtering \\
			\hline 
			0 & 216595 & 212129 \\
			1 & 212729 & 207697 \\
			\hline    
		\end{tabular}
		\label{table:numberEmbeddingsClasses}
	\end{table}

	\newpage

	\subsection{Analysed representations}
	\label{analysedRepresentations}

	As a consequence of the former work, this thesis includes 
	analysis of ProtTrans protein language model's mean embeddings 
	usage in protein classification. 

	Besides the original mean embeddings, principal components 
	of ESM-1b and ProtTrans
	mean embeddings were retrieved and taken as input for the 
	model. In particular, principal components that explained 
	95 percent and 100 percent of the data variance were picked.

	\begin{table}[h!]
		\caption{Sizes of the analysed principal components 
		vectors}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Explained variance & ESM-1b & ProtTrans \\
			\hline 
			95\% & 540 & 453 \\
			100\% & 1280 & 1024 \\
			\hline    
		\end{tabular}
		\label{table:vectorsPCADimensions}
	\end{table}
	
	Furthermore, both protein language models - ESM-1b and 
	ProtTrans - provide 
	per token or per residue representations - each 
	amino acid of the protein gets a 1280 or 1024-dimensional vector from
	ESM-1b or ProtTrans model respectively. Therefore, each protein is 
	originally represented by the ${m \times n}$ matrix, 
	where ${m}$ is the number of dimensions of the chosen type of embedding
	and ${n}$ is the number of amino acids that compose the protein. 
	These representations are processed to get vectors with the same 
	dimension 
	for each protein in the data set. The representations 
	included in the analysis were:

	\begin{enumerate}
		\item Mean ESM-1b and ProtTrans 
		\item Joined mean ESM-1b and ProtTrans
		\item Normalised mean ESM-1b and ProtTrans
		\item Joined normalised mean ESM-1b and ProtTrans
		\item Median ESM-1b and ProtTrans
		\item Minimum, median, and maximum ESM-1b and ProtTrans
		\item Quantiles (including minimum and maximum) ESM-1b and ProtTrans
		\item Quantiles (including minimum and maximum) and mean ESM-1b and ProtTrans
		\item Octiles (including minimum and maximum) ESM-1b and ProtTrans
	\end{enumerate}

    \newpage

	\begin{table}[h!]
		\caption{Sizes of the analysed representations' vectors}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Representation & ESM-1b & ProtTrans \\
			\hline 
			Mean & 1280 & 1024 \\
			Joined mean & \multicolumn{2}{c|}{2304} \\
			Median & 1280 & 1024 \\
			Minimum, median, maximum & 3840 & 3072 \\
			Quantiles & 6400 & 5120 \\
			Quantiles and mean & 7680 & 6144 \\
			Octiles & 11520 & 9216 \\
			\hline    
		\end{tabular}
		\label{table:vectorsDimensions}
	\end{table}

	\newpage

	\subsection{Analysed architectures}

	All representations that were described
	in the previous section were taken as input for the baseline
	single-layer perceptron models, although another important part 
	of this work was to explore several different 
	model architectures. 

	Architectures that were chosen to run experiments with 
	had one or two hidden layers. The sizes of hidden layers 
	were chosen to be original embeddings size divided 
	by several multiples of 2 (Tables \ref{table:modelArchitecturesESM} 
	and \ref{table:modelArchitecturesPT}). That is, since the 
	size of the ESM-1b embedding is 1280, there were 
	models defined with one hidden layer of size 640, 320, or 160 
	that take ESM-1b embeddings as input. For models with two 
	hidden layers, sizes of hidden layers were assigned by 
	combining two sequential sizes received from division. 
	Analogously, the same operation was done for models adjusted 
	for ProtTrans.

	\begin{table}[h!]
		\caption{Models that were tested with ESM-1b embeddings input}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Model & Number of hidden layers & Size of hidden layers \\
			\hline 
			C2H2\_h640-320 & 2 & 640, 320 \\
			C2H2\_h320-160 & 2 & 320, 160 \\
			C2H1\_h640 & 1 & 640 \\
			C2H1\_h320 & 1 & 320 \\
			C2H1\_h160 & 1 & 160 \\
			SLP\_ESM-1b & 0 & - \\
			\hline    
		\end{tabular}
		\label{table:modelArchitecturesESM}
	\end{table}

	\begin{table}[h!]
		\caption{Models that were tested with ProtTrans embeddings input}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c | }
			\hline 
			Model & Number of hidden layers & Size of hidden layers \\
			\hline 
			C2H2\_h512-256 & 2 & 512, 256 \\
			C2H2\_h256-128 & 2 & 256, 128 \\
			C2H1\_h512 & 1 & 512 \\
			C2H1\_h256 & 1 & 256 \\
			C2H1\_h128 & 1 & 128 \\
			SLP\_ProtTrans & 0 & - \\
			\hline    
		\end{tabular}
		\label{table:modelArchitecturesPT}
	\end{table}

	\newpage

	\section{Results}

	\subsection{Correlation analysis of embeddings' components}

	One of the tasks of this work was to try joined ESM-1b and ProtTrans 
	representations and pass them as input to the thermostability 
	classification model. Additionally, the correlation 
	coefficients between the components of these embeddings were analysed. 

	The results of the analysis showed that there are five ESM-1b
	embeddings' components that have absolute correlation coefficients 
	higher than 0.5 with more than 10 ProtTrans embeddings' components  
	(Figure \ref{figure:highCorrelationComponents}). However, overall the 
	majority of components' pairs had correlation coefficients close to zero 
	(Figure \ref{figure:correlationComponentsHisto}).

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_correlation_high_corr.png}

		\caption{Plot of ESM-1b components that have got high absolute correlation 
		coefficients with ProtTrans components}
		\label{figure:highCorrelationComponents}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_correlation_hist.png}

		\caption{Histogram of correlation coefficients between ESM-1b 
		and ProtTrans components}
		\label{figure:correlationComponentsHisto}
	\end{figure}

	\newpage

	The plot that visualises absolute maximum correlation coefficients between 
	pairs of components has two curves (Figure \ref{figure:correlationComponentsMaxAndMean}). 
	The grey curve provides raw coefficient correlation values - there are peaks 
	at ESM-1b positions that correspond with positions that have the 
	biggest number of high (absolute value above 0.5) correlation coefficients
	(Figure \ref{figure:highCorrelationComponents}). The blue curve shows the trend 
	of absolute maximum values of correlation coefficients along the ESM-1b components.
	None of the peaks of the trend curve overstep 0.5. Therefore, it can 
	be concluded that there are no intervals of ESM-1b components that have 
	a considerably high correlation with ProtTrans components.

	The observations that can be made from the plot of averaged correlation 
	coefficients (Figure \ref{figure:correlationComponentsMaxAndMean}) do not change the 
	overview of the correlation between 
	ESM-1b and ProtTrans components - the trend of mean correlation coefficients 
	fluctuates around 0.1.

	\begin{figure}
		\centering
		\begin{tabular}{@{}c@{}}
			\includegraphics[scale=0.85]{validation_small_set_2_joined_correlation_max.png}
		\end{tabular}

		\begin{tabular}{@{}c@{}}
			\includegraphics[scale=0.85]{validation_small_set_2_joined_correlation_mean.png}
		
		\end{tabular}
		
		\caption{Plots of ESM-1b components' maximum and mean absolute correlation coefficients 
		with ProtTrans components}\label{figure:correlationComponentsMaxAndMean}
	\end{figure}

	\newpage

	Additionally, it was attempted to analyse correlation coefficients between 
	embeddings' principal components that explain 95 percent of data variation.

	The analogous scatter plot of the number of absolute correlation 
	coefficients higher than 0.5 was drawn
	(Figure \ref{figure:highCorrelationComponentsPC95}). The plot 
	demonstrated that there are few high correlation coefficients 
	between components' pairs overall and this observation is supported 
	by the histogram of correlation coefficients 
	(Figure \ref{figure:correlationComponentsHistoPC95}). Note that the 
	number of correlation coefficients for principal components' correlation 
	analysis was around five times smaller than in the raw embeddings' components
	analysis (Table \ref{table:vectorsPCADimensions}).

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_PC_95_correlation_high_corr.png}

		\caption{Plot of ESM-1b principal components (explaining 
		95\% of data variation) that have got high correlation coefficients 
		with ProtTrans principal components (95\%)}
		\label{figure:highCorrelationComponentsPC95}
	\end{figure}

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.85]{validation_small_set_2_joined_PC_95_correlation_hist.png}

		\caption{Histogram of correlation coefficients between ESM-1b 
		and ProtTrans principal components (95\%)}
		\label{figure:correlationComponentsHistoPC95}
	\end{figure}

	\newpage

	The curve of absolute maximum correlation coefficients shows 
	that there is a trend of correlation coefficients to decrease 
	as the index of ESM-1b component is increasing 
	(Figure \ref{figure:correlationComponentsMaxAndMeanPC95}). The following 
	plot of mean correlation coefficients 
	(Figure \ref{figure:correlationComponentsMaxAndMeanPC95}) supports the 
	statement that high maximum values of absolute correlation 
	coefficients are not dominating 
	because the mean correlation is very small between the pairs.

	\begin{figure}
		\centering
		\begin{tabular}{@{}c@{}}
			\includegraphics[scale=0.85]{validation_small_set_2_joined_PC_95_correlation_max.png}
		\end{tabular}

		\begin{tabular}{@{}c@{}}
			\includegraphics[scale=0.85]{validation_small_set_2_joined_PC_95_correlation_mean.png}
		\end{tabular}
		
		\caption{Plot of ESM-1b principal components' (95\%) mean and maximum correlation 
		coefficients with ProtTrans principal components (95\%)}
		\label{figure:correlationComponentsMaxAndMeanPC95}
	\end{figure}

	\newpage

	\subsection{Representation analysis}

	The first step of the analysis was to try ProtTrans embeddings 
	as input for the SLP model and compare 
	its results with the testing metrics of model trained with 
	ESM-1b. For the comparison, the primary model with ESM-1b was 
	retrained using the filtered data set. The comparison disclosed 
	that the model that uses ProtTrans embeddings as input performs 
	better (Table \ref{table:comparisonESMandPTmeanNormMeanJoined}).

	Out of the scope of this representation and model architecture analysis
	for binary classification, there were several attempts made to overtrain
	the classification model to predict three thermostability classes (one 
	more class added after dividing the zero-labelled class at the temperature
	threshold of 40 degrees Celsius). The purpose of overtraining was to check 
	whether the selected architecture has a potential to be trained for the 
	multiclass classification problem. The usage of principal 
	components of protein embeddings showed that overfitting can be 
	done successfully.

	Therefore, it was decided to check whether 
	a vector of principal components could be a suitable input for the binary 
	classification task. The testing stage metrics showed worse model's performance 
	than using the original representations (Tables \ref{table:comparisonESMandPTunfiltMeanPC}). 

	\begin{table}[h!]
		\caption{The comparison of scores between models trained with ESM-1b
		and ProtTrans mean representations and their principal components 
		that account for 95\% and 100\% of the unfiltered data set variance}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c c c c c | }
			\hline 
						
			& \specialcell{Mean\\ESM-1b} & \specialcell{Mean\\ProtTrans} & \specialcell{ESM-1b\\(95\%)} & \specialcell{ProtTrans\\(95\%)} & \specialcell{ESM-1b\\(100\%)} & \specialcell{ProtTrans\\(100\%)} \\
			\hline 
			MCC & 0.843 & 0.902 & 0.699 & 0.767 & 0.698 & 0.766 \\
			Accuracy  & 0.922 & 0.951 & 0.845 & 0.880 & 0.845 & 0.879 \\
			Loss & 0.208 & 0.128 & 0.383 & 0.442 & 0.382 & 0.443 \\
			Precision & 0.919 & 0.949 & 0.910 & 0.940 & 0.909 & 0.939 \\
			Recall & 0.921 & 0.951 & 0.768 & 0.813 & 0.767 & 0.812 \\
			ROC AUC & 0.979 & 0.990 & 0.901 & 0.945 & 0.901 & 0.943 \\

			\hline    
		\end{tabular}
		\label{table:comparisonESMandPTunfiltMeanPC}
	\end{table}

	\newpage

	Before joining the embeddings, the normalisation of ESM-1b and ProtTrans 
	vectors was done. Normalised representations were taken as input to 
	the model with the same SLP architecture. For both types of embeddings 
	the results were improved (Table \ref{table:comparisonESMandPTmeanNormMeanJoined}).

	After joining ESM-1b and ProtTrans mean embeddings, an SLP was trained 
	using these joined representations. The results of this model were similar 
	to the scores of the model that was trained using only ProtTrans embeddings, 
	though the results 
	did not improve (Table \ref{table:comparisonESMandPTmeanNormMeanJoined}). 

	\begin{table}[h!]
		\caption{The comparison of testing stage scores between 
		models trained with mean, normalised mean, joined mean, 
		and normalised joined ESM-1b and ProtTrans 
		representations}
		\vspace{0.2cm}
		\centering
		\begin{tabular}{ | c | c c c c c c | }
			\hline 
						
			& ESM-1b & \specialcell{Normalised\\ESM-1b} & ProtTrans & \specialcell{Normalised\\ProtTrans} & Joined & \specialcell{Normalised\\joined} \\
			\hline 
			MCC & 0.843 & 0.858 & 0.901 & 0.915 & 0.899 & 0.920 \\
			Accuracy & 0.921 & 0.929 & 0.951 & 0.957 & 0.949 & 0.960 \\
			Loss & 0.208 & 0.248 & 0.128 & 0.143 & 0.131 & 0.139 \\
			Precision & 0.921 & 0.923 & 0.949 & 0.951 & 0.945 & 0.954 \\
			Recall & 0.917 & 0.931 & 0.949 & 0.962 & 0.951 & 0.964 \\
			ROC AUC & 0.979 & 0.982 & 0.990 & 0.991 & 0.991 & 0.992 \\
			\hline    
		\end{tabular}
		\label{table:comparisonESMandPTmeanNormMeanJoined}
	\end{table}

	However, joining the normalised ESM-1b and ProtTrans mean representations 
	showed the best results. Since joined representations require
	generation of ESM-1b embeddings, this type of representation does not 
	solve the length limitation problem that was noticed in the previous work. 
	Nevertheless slightly improved results can be observed when normalised 
	representations are used, the process of normalisation depends on the data 
	set, which is not convenient in the process of development until the 
	final data set is established. Therefore, the optimal choice for 
	this stage of development was mean ProtTrans embeddings.

	Nonetheless, ProtTrans already demonstrated the impact for the 
	model's improvement on the performance, it was decided to finish up 
	the different representation and architecture analysis using embeddings
	of both protein language models for completeness. The results 
	of the consequent analysis 
	did not change the conclusion regarding ProtTrans influence for the 
	results using any variation of analysed representations (listed in the
	section \ref{analysedRepresentations}) - in all cases model that took 
	ProtTrans embeddings as input performed significantly better (Figures 
	\ref{figure:scoresRepresentationsESM} and 
	\ref{figure:scoresRepresentationsPT}).

	\newpage

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.7]{SLP_ESM_003_diff_representations.png}

		\caption{Comparison of SLP models', which were trained with different
		ESM-1b representations, MCC and ROC AUC scores}
		\label{figure:scoresRepresentationsESM}
	\end{figure}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.7]{SLP_PT_003_diff_representations.png}

		\caption{Comparison of SLP models', which were trained with different
		ProtTrans representations, MCC and ROC AUC scores}
		\label{figure:scoresRepresentationsPT}
	\end{figure}

	\newpage

	\subsection{Architecture analysis}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.6]{MLP_ESM.png}

		\caption{Comparison of models', which were trained using 
		ESM-1b embeddings, MCC and ROC AUC scores}
		\label{figure:scoresMLP_ESM}
	\end{figure}

	\begin{figure}[h!]
		\centering
		\includegraphics[scale=0.6]{MLP_PT.png}

		\caption{Comparison of models', which were trained using 
		ProtTrans embeddings, MCC and ROC AUC scores}
		\label{figure:scoresMLP_PT}
	\end{figure}

	\newpage

	\section{Conclusions}

	The results of this work provided following conclusions: for 
	further development of the final method ProtTrans mean and 
	octiles embeddings will be used. Additionally, this work 
	showed that it is worth to use the model's architecture with 
	two hidden layers with sizes 512 and 256.

	\section{Availability}

	The code that was used to receive the results of this work can be found
	in the designated Github repository: 
	\href{https://github.com/ievapudz/Course_Work_Project}{https://github.com/ievapudz/Course\_Work\_Project}.

	\newpage
	
	\nocite{*}
	
	\normalsize

\bibliography{references} 
\bibliographystyle{ieeetr}

\end{document}
